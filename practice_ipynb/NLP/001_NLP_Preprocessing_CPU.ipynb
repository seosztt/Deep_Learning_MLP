{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "001_NLP_Preprocessing_CPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seosztt/Deep_Learning_MLP/blob/master/practice_ipynb/NLP/001_NLP_Preprocessing_CPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639BJgw2C9qL"
      },
      "source": [
        "# NLP Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbPgODpVzdHI"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh_HVEagpB_D"
      },
      "source": [
        "# I. Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kAGmHLXDJAp"
      },
      "source": [
        "> ## 1) 영어 : NLTK(Natural Language ToolKit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDoUfry3Bemz",
        "outputId": "d6a61373-1a7c-444c-8feb-6b336fe0245a"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVBO3ypqYAF"
      },
      "source": [
        "> ### (1) 문장 토큰화 : sent_tokenize( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7vJH0SjqcaI",
        "outputId": "7b1f9404-4240-4e2b-f888-4a950145bc0c"
      },
      "source": [
        "from nltk import sent_tokenize\n",
        "\n",
        "sentences = 'The X-Files is an American science fiction drama television series \\\n",
        "created by Chris Carter. \\\n",
        "             The original television series aired from September 10, 1993 \\\n",
        "to May 19, 2002 on Fox. \\\n",
        "             The program spanned nine seasons, with 202 episodes.'\n",
        "\n",
        "sent_tokenize(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The X-Files is an American science fiction drama television series created by Chris Carter.',\n",
              " 'The original television series aired from September 10, 1993 to May 19, 2002 on Fox.',\n",
              " 'The program spanned nine seasons, with 202 episodes.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjrg-06NDbbQ"
      },
      "source": [
        "> ### (2) 단어 토큰화 : word_tokenize( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yvJOSQX-0Sf"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-icwNGL_kzm",
        "outputId": "d7c1d3e2-bdeb-4e1a-89ea-b65ae055769b"
      },
      "source": [
        "text = 'The truth is out there.'\n",
        "\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'truth', 'is', 'out', 'there', '.']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crB1Uea0D4sN"
      },
      "source": [
        "> ### (3) 단어 품사(Part Of Speech) 태깅 : pos_tag( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU5LqsjBCBC6"
      },
      "source": [
        "from nltk.tag import pos_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLBKvue0CJwe",
        "outputId": "46898146-1fec-48a8-a894-d5f334db1f3e"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0XzmeyzCDFy",
        "outputId": "0ec908ee-4d1a-4d80-a9d0-73a1da2d9e98"
      },
      "source": [
        "x = word_tokenize(text)\n",
        "\n",
        "pos_tag(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('truth', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('out', 'RP'),\n",
              " ('there', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8VBMU1GwTZz"
      },
      "source": [
        "> ### (4) Stop Words\n",
        "\n",
        "* Import Package and Download 'stopwords'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbNN_2cLwZJB",
        "outputId": "a1fa5490-f184-4e37-8dfe-54e4f24bef45"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfnVbP93zyd2"
      },
      "source": [
        "* 'English' Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orHJ8CWKwcXm",
        "outputId": "fbe7eb95-b05c-45e5-d231-b9bfa6921063"
      },
      "source": [
        "print('English stop words :',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English stop words : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQIfsKIPz57j"
      },
      "source": [
        "* tokenize_text( ) 정의\n",
        " - 여러개의 문장별로 단어 토큰 생성 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYA4mJx_xFHf"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(doc):\n",
        "    sentences = sent_tokenize(doc)\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return word_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UKRicCO0U63"
      },
      "source": [
        "* 문장별 단어 토큰화 수행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "964fOeOa0Rzx",
        "outputId": "3efc0047-4245-4e34-cdb7-3a6a99c80c7b"
      },
      "source": [
        "word_tokens = tokenize_text(sentences)\n",
        "\n",
        "print(type(word_tokens),len(word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbYBIBf20fBe"
      },
      "source": [
        "* 문장별 단어 토큰화 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7vfFzOKzWZd",
        "outputId": "e5fa97dd-3a7b-49f0-f757-7f9aad926d43"
      },
      "source": [
        "print(word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['The', 'X-Files', 'is', 'an', 'American', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'by', 'Chris', 'Carter', '.'], ['The', 'original', 'television', 'series', 'aired', 'from', 'September', '10', ',', '1993', 'to', 'May', '19', ',', '2002', 'on', 'Fox', '.'], ['The', 'program', 'spanned', 'nine', 'seasons', ',', 'with', '202', 'episodes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwQKZmKn0xVx"
      },
      "source": [
        "* Stop Words 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnVIc_GhxvqE"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens:\n",
        "    filtered_words = []\n",
        "\n",
        "    for word in sentence:\n",
        "        word = word.lower()\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    all_tokens.append(filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf9rcBws1Vjt"
      },
      "source": [
        "* Stop Words 처리 결과"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXa_ACOI09Hc",
        "outputId": "598796b9-b9f7-4505-a89a-fca698c1c3f3"
      },
      "source": [
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['x-files', 'american', 'science', 'fiction', 'drama', 'television', 'series', 'created', 'chris', 'carter', '.'], ['original', 'television', 'series', 'aired', 'september', '10', ',', '1993', 'may', '19', ',', '2002', 'fox', '.'], ['program', 'spanned', 'nine', 'seasons', ',', '202', 'episodes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpC_pY631aPG"
      },
      "source": [
        "> ### (5) Stemming(어간 추출)\n",
        "\n",
        "* 변화된 단어의 원형으로 처리\n",
        " - work\n",
        " - amuse\n",
        " - happy\n",
        " - fancy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK1KB0y21lo5",
        "outputId": "04338896-67af-4782-fdc1-35f32c3fd177"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUPYMA21t5G"
      },
      "source": [
        "> ### (6) Lemmatization(표제어 추출)\n",
        "\n",
        "* 변화된 단어의 원형을 처리\n",
        " - Stemming 보다 정확한 처리 가능\n",
        " - '품사'를 지정하여 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4emYOj_1yxT",
        "outputId": "84f3e14d-f816-4a41-97cb-6d5aa9f2683d"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N1cStbG3jul"
      },
      "source": [
        "* 'v' 동사, 'a' 형용사"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CBmXLfC3Fh9",
        "outputId": "927fe72b-8e6d-4bad-bbc1-87453ae8859b"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), \\\n",
        "      lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ACEmS8vEMmA"
      },
      "source": [
        "> ## 2) 한국어 : KoNLPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwY2Mf31ETrB"
      },
      "source": [
        "> ### (1) KoNLPy 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCTvfuOlCdze",
        "outputId": "06a80d9f-883e-45ef-b8e2-30a2c141cd1c"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 59.0 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BMkvJxjEYb5"
      },
      "source": [
        "> ### (2) Okt 형태소 분석기(Open Korea Text, Twitter)\n",
        "\n",
        "* 형태소(Morpheme)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H33ddPMFCWjS",
        "scrolled": true
      },
      "source": [
        "from konlpy.tag import Okt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx0j3jE0FKLX"
      },
      "source": [
        "* 토큰화 : okt.morphs( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5A_yZxTCcYC",
        "outputId": "b75885c5-a829-410c-fe09-568809d3331c"
      },
      "source": [
        "okt = Okt()\n",
        "\n",
        "print(okt.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지난', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인해', '전례', '없는', '고통', '을', '겪으며', '다양한', '방식', '으로', '심각하게', '피해', '를', '겪었습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgwn4Pk5FN76"
      },
      "source": [
        "* 품사 태깅 : okt.pos( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpR6zpEMCp-c",
        "outputId": "8fe45b11-d376-4e28-bd95-8a6e13b85285"
      },
      "source": [
        "print(okt.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('지난', 'Noun'), ('몇', 'Noun'), ('달', 'Noun'), ('간', 'Suffix'), ('전', 'Noun'), ('세계', 'Noun'), ('모든', 'Noun'), ('사람', 'Noun'), ('은', 'Josa'), ('코로나', 'Noun'), ('19', 'Number'), ('로', 'Noun'), ('인해', 'Adjective'), ('전례', 'Noun'), ('없는', 'Adjective'), ('고통', 'Noun'), ('을', 'Josa'), ('겪으며', 'Verb'), ('다양한', 'Adjective'), ('방식', 'Noun'), ('으로', 'Josa'), ('심각하게', 'Adjective'), ('피해', 'Noun'), ('를', 'Josa'), ('겪었습니다', 'Verb'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Y4CYCYFjL5"
      },
      "source": [
        "* 명사 추출 : okt.nouns( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqd_VBtbCri0",
        "outputId": "30d92837-bd14-4be7-bf89-3016c194892a"
      },
      "source": [
        "print(okt.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지난', '몇', '달', '전', '세계', '모든', '사람', '코로나', '로', '전례', '고통', '방식', '피해']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxJAG2fSE7ua"
      },
      "source": [
        "> ### (3) Kkma 형태소 분석기\n",
        "\n",
        "* 형태소(Morpheme)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkj_276WCxDQ"
      },
      "source": [
        "from konlpy.tag import Kkma"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIVbvbClFtUD"
      },
      "source": [
        "* 토큰화 : kkma.morphs( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slbdMEw_FvV1",
        "outputId": "544d784a-685f-4d50-aaa7-63ee2fc5cdab"
      },
      "source": [
        "kkma = Kkma()  \n",
        "\n",
        "print(kkma.morphs('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['지나', 'ㄴ', '몇', '달', '간', '전', '세계', '모든', '사람', '은', '코로나', '19', '로', '인하', '어', '전례', '없', '는', '고통', '을', '겪', '으며', '다양', '하', 'ㄴ', '방식', '으로', '심각', '하', '게', '피해', '를', '겪', '었', '습니다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VuXMtjEGA6r"
      },
      "source": [
        "* 품사 태깅 : kkma.pos( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o__YXyPTCzBb",
        "outputId": "ce5cb6d3-501e-44c4-ea1e-4432765c0155"
      },
      "source": [
        "print(kkma.pos('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('지나', 'VV'), ('ㄴ', 'ETD'), ('몇', 'MDT'), ('달', 'NNG'), ('간', 'NNB'), ('전', 'NNG'), ('세계', 'NNG'), ('모든', 'MDT'), ('사람', 'NNG'), ('은', 'JX'), ('코로나', 'NNG'), ('19', 'NR'), ('로', 'JKM'), ('인하', 'VV'), ('어', 'ECS'), ('전례', 'NNG'), ('없', 'VA'), ('는', 'ETD'), ('고통', 'NNG'), ('을', 'JKO'), ('겪', 'VV'), ('으며', 'ECE'), ('다양', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('방식', 'NNG'), ('으로', 'JKM'), ('심각', 'XR'), ('하', 'XSA'), ('게', 'ECD'), ('피해', 'NNG'), ('를', 'JKO'), ('겪', 'VV'), ('었', 'EPT'), ('습니다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TolIm0ZjGFXE"
      },
      "source": [
        "* 명사 추출 : kkma.nouns( )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYTaUE7DC2bb",
        "outputId": "f5c2e364-e71a-4ef0-da49-2b4d99aca683"
      },
      "source": [
        "print(kkma.nouns('지난 몇 달간 전 세계 모든 사람은 코로나19로 인해 \\\n",
        "전례 없는 고통을 겪으며 다양한 방식으로 심각하게 피해를 겪었습니다.'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['달', '달간', '간', '전', '세계', '사람', '코로나', '코로나19', '19', '전례', '고통', '다양', '방식', '피해']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Br06_7LLIv"
      },
      "source": [
        "# II. Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBO0Jsk1LPKl"
      },
      "source": [
        "> ## 1) Encoding with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10-0Yzx1LayX"
      },
      "source": [
        "> ### (1) Import Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgw2BcXHKCjv"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9gFk2zqLlIp"
      },
      "source": [
        "> ### (2) 실습 문장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml7ZtnR5Ke4_"
      },
      "source": [
        "sentence = '가지마라 가지마라 그녀는 위험해 매력이 너무 넘치는 Girl \\\n",
        "하지마라 하지마라 사랑은 위험해 \\\n",
        "내가 내가 내가 먼저 네게 네게 네게 빠져 빠져 빠져 버려 baby'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPe_IJ4VLoin"
      },
      "source": [
        "> ## 2) 정수 인코딩(Integer Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck2RxCHdL9Xc"
      },
      "source": [
        "> ### (1) Tokenizer.fit_on_texts( )\n",
        "\n",
        "* Tokenization & Integer Indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdvsdKLzKj9_"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tknz = Tokenizer()\n",
        "tknz.fit_on_texts([sentence])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re3hcM1pva9D",
        "outputId": "02941e69-4254-458c-c054-f8d032d053d4"
      },
      "source": [
        "print(tknz.word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'내가': 1, '네게': 2, '빠져': 3, '가지마라': 4, '위험해': 5, '하지마라': 6, '그녀는': 7, '매력이': 8, '너무': 9, '넘치는': 10, 'girl': 11, '사랑은': 12, '먼저': 13, '버려': 14, 'baby': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_foET2ciMJbW"
      },
      "source": [
        "> ### (2) Tokenizer.texts_to_sequences( )\n",
        "\n",
        "* Integer Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZbWP8G9KovQ"
      },
      "source": [
        "LBE = tknz.texts_to_sequences([sentence])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AeATrVDvnga",
        "outputId": "70df43fb-48cd-4683-d561-a65dfe973a37"
      },
      "source": [
        "print(LBE)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 4, 7, 5, 8, 9, 10, 11, 6, 6, 12, 5, 1, 1, 1, 13, 2, 2, 2, 3, 3, 3, 14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsdDOxqDLupv"
      },
      "source": [
        "> ## 3) 원-핫 인코딩(One-Hot Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-FQ-mZhL2wV"
      },
      "source": [
        "> ### (1) to_categorical( )\n",
        "\n",
        "* One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGQqaQxiLBQr"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "OHE = to_categorical(LBE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3DzE25Rvufj",
        "outputId": "55768997-8584-4582-936d-01608c4a7512"
      },
      "source": [
        "print(OHE)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO4ANdxgGJ10"
      },
      "source": [
        "# \n",
        "# \n",
        "# \n",
        "# The End\n",
        "# \n",
        "# \n",
        "# "
      ]
    }
  ]
}